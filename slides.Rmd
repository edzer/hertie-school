---
title: "GeoData and Spatial Data Analysis with R"
author: "Edzer Pebesma"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 2
---

```{r, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

Slides prepared for the [Data Science Summer
School](https://socialdatascience.network/summerschool.html),
Jul 30, 2021; see also the [course
page](https://socialdatascience.network/courses/geodata.html).

Program: 

* 10:00 AM GeoData and Spatial Data Analysis with R (Part I)
* 12:00 PM Short Break
* 12:15 PM GeoData and Spatial Data Analysis with R (Part II)
* 02:15 PM end

Each full hour will have about 45 minutes lecturing, followed by
15 mins Q&A.

# Context

* **Why "GeoData and Spatial Data"?** The organisation chose
that title. I consider "GeoData" and "Spatial Data"
as equal. "Geo" refers to "Earth", so some spatial data,
e.g. astronomic images or images taken through microscopes or
MRT scans are less Earth-bound, and might be better described as
"Spatial Data".
* **Why "Data Analysis?"** Because this is a summer school on Data Science.
* **Why with R?**  Because R is an open source environment **made for
data analysis and graphics** that **runs on all platforms**. If we'd use
a closed source environment, we'd miss the opportunity to reproduce and scrutinize
computations and fail to meet the _science_ goal of _Data Science_;
if we'd use a general purpose programming language we would (more)
easily end up in a package installation hell.
* **Why me?** I have been involved in R, and development of R-Spatial
packages, for around 20 years, teach this subject on a regular basis,
and recently finished the draft of the book [Spatial Data Science
(with applications in R)](https://r-spatial.org/book).

## How to use these slides?

These slides were created with R and the R package `rmarkdown`. The
source of these slides is an R-markdown document that can be
loaded in R, and _executed_ there (just like a Jupyter notebook,
but simpler). To do this, take the following steps:

1. Go to the [GitHub page](https://github.com/edzer/hertie-school) of this course
2. Click on the file `slides.Rmd`
3. Click on the "Raw" tab
4. Right-click on that page, "save as", and save it to a local copy of `slides.Rmd`
5. Click (or double-click) on this file, and RStudio should open it, showing the file
6. In RStudio, click "knit" to recreate the entire rendered document, which _runs all the R chunks_
7. For running individual R chunks, (notebook "cells"), use the arrows "Run all chunks above", after which you can use "Run current chunk"

## Packages needed

All packages needed can be installed with

```{r eval=FALSE}
install.packages(c("gapminder", "gstat", "maps", "mapview",
  "rnaturalearth", "sf", "spatstat", "stars", "stringr", "tidyverse",
  "units")
```

## What to expect?

10-11: Spatial data and geometries

* What is so special about spatial data?
* How can we represent spatial data in R?
* What does a coordinate mean?
* What does _coordinate reference system_ mean?
* Geometries, measures, predicates, transformers
* Q&A

11-12: Attributes

* Support, aggregates
* `gapminder` example

12:15-13:15: Data cubes, large datasets

* `gapminder` data cube
* raster data
* raster time series
* image data cubes, image collections

13:15-14:15: Models

* point patterns
* geostatistical data
* lattice data

# Spatial data and geometries

## Special about spatial data

**All data are spatial** - data comes from observation, and
observation needs to happen _somewhere_ and at _some time_.
This makes all data spatial. For a lot of data, the location
expressed in spatial, Earth-bound coordinates of observation is
not of prime importance:

* if a patient undergoes a brain scan, the location of the scanner
is not important; the location of the person relative to the
scanner is
* if a person receives a positive COVID-19 test result, the location
of testing may not be important for the person's decision on whether
to go into quarantine or not
* for someone trying to do contact tracing, this person's location
history may be most relevant, however

With spatial data we mean data for which spatial locations
(or relations) are known, and for which they play a role in the
exploration, analysis or visualisation of the data.

## Representing spatial data in R

Several of the simple R data structures can be used to represent spatial
data, examples are **columns in a data.frame**:

```{r}
head(quakes)
```

**a matrix**

```{r}
class(volcano)
volcano[1:5,1:5]
image(volcano)
```

**numeric vectors for polygon coordinates** with `NA` to separate individual rings:

```{r}
library(maps)
m = map(regions="Australia", plot = FALSE, fill = TRUE)
pols = cbind(m$x, m$y)
dim(pols)
head(cbind(m$x, m$y), 10)
map(regions="Australia")
```

We will shortly see a few more structured approaches to represent
spatial data, using _simple features_, _coverages_ or _rasters_.
First let us look at what coordinates mean.

## Coordinates

With coordinates, we usually think a numbered measured along a ruler,
where the ruler might be an imaginary line: it has an offset (0),
a unit (m), and a constant direction. For spatial data we could have two
imageginary lines perpendicular to each other, and we call this
Cartesian space. Distance between $(x_1,y_1)$ and
$(x_2,y_2)$ in Cartesian space is computed by Euclidean distance:
$$\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$$

The spatial data we just saw, above are _not_ of this kind:

* the raster data (`volcano`) did _not_ have coordinates
* the other examples had coordinates that are **ANGLES**,
  distances along a circle (or ellipsoidal) arc:

```{r out.width='100%',echo=FALSE}
knitr::include_graphics("https://keen-swartz-3146c4.netlify.app/sds_files/figure-html/sphere-1.png")
```
Left: geocentric coordinates (Cartesian, three-dimensional,
units metres); Right: spherical/ellipsoidal coordinates (angles,
units degrees)

Euclidean distances **do not work** for ellipsoidal coordinates:
one degree longitude at the equator is about 111 km, at the poles
it is 0 km.

## What does _coordinate reference system_ mean?

"Data are not just numbers, they are numbers with a context" ([Cobb &
Moore](https://www.jstor.org/stable/2975286?seq=1#metadata_info_tab_contents))

Coordinate reference systems provide the context of coordinates:

* they tell whether the coordinates are ellipsoidal (angles), or derived, projected (Cartesian) coordinates
* in case they are projected, they detail the kind of projection used, so that the
underlying ellipsoidal coordinates can be recovered
* in **any** case, they point out which ellipsoidal model (datum) was used.

Knowing this we can

* convert between projected and unprojected, or to another projection
* transform from one datum to another
* combine the coordinates with any other coordinates that have a coordinate reference system

## Geometries, measures, predicates, transformers

Geometries can be described in many ways. We use the [simple feature
access]() specification that focuses on points, lines and polygons;
the main types are:

* points, `POINT(0 1)`
* sets of points, `MULTIPOINT(0 1,10 8)`
* linestrings, `LINESTRING(0 0,1 1)`
* sets of linestrings, `MULTILINESTRING((0 0,1 1),(5 5,4 6))`
* polygons, `POLYGON((0 0,1 0,1 1,0 0))`
* sets of polygons, `MULTIPOLYGON(((0 0,1 0,1 1,0 0)), ((3 3,4 3,4 4,3 3)))`
* combinations (mixes) of these `GEOMETRYCOLLECTION(POINT(0 1),LINESTRING(0 0,1 1))`

Linestrings are formed by sequences of points, where straight
lines are thought of as connecting them. Polygons are formed of
non self-intersecting linestrings that form closed rings (first
coordinate equals last coordinate).

An example:
```{r}
library(sf)
(nc = read_sf(system.file("gpkg/nc.gpkg", package="sf")))
library(ggplot2)
ggplot() + geom_sf(data = nc, aes(fill = SID74))
```

## Coverages, rasters

Coverages are tesselations (subdivisions) of space into regions,
where _every_ point can be uniquely assigned to a subregion. 

With `POLYGONS` we cannot do that, as two polygons that share
a boundary cannot tell to which polygon a point on the shared
boundary belongs.

Rasters are _regular_ tesselations, which uniquely subdivide space
into square or rectangular areas (raster cells, or pixels).

An example of a raster:
```{r}
library(stars)
(L7 = read_stars(system.file("tif/L7_ETMs.tif", package = "stars")))
ggplot() + geom_stars(data = L7) + facet_wrap(~band) + coord_equal()
```

or simply:
```{r}
plot(L7)
```

## Q&A

# Attributes, support

Spatial data gets more interesting when we not only consider the
geometric properties, but also other properties, which we call
(feature) attributes. 

A crucial consideration here is _support_. Non-point geometries
(lines, polygons) contain essentially an infinite number of points.
If we have an attribute of such a geometry, the question is whether
the attribute is valid for

* each point in the geometry ("point support")
* the collection of points ("line/area/block support")

An example of point support: if we have a polygon of a soil map
delineating the area with sandy soils, it indicates (up to mapping
errors) that _every_ point in the polygon has a sandy soil.

An example of area support: if we have an administrative region,
the population density of that region is not a density measure for
_every_ point in the area, but an _aggregate_ value relevant for
the polygon as a whole.

When we analyse area support data (aggregates), choosing
a different set of polygons leads to different results,
a phenomenon known as the _modifiable areal unit problem_
([MAUP](https://en.wikipedia.org/wiki/Modifiable_areal_unit_problem)).
Intentially manipulating polygons such that a particular
analysis result is obtained, e.g. in election outcomes, is called
[gerrymandering](https://en.wikipedia.org/wiki/Gerrymandering).

A common operation on areal data is to generate attributes for a
new spatial subdivision, often indicated by

* _upscaling_  when the new units are larger
* _downscaling_ when the new units are smaller

When doing this, for spatially _extensive_ variables (e.g. population
counts) their _sum_ needs to be preserved, for spatially _intensive_
variables (e.g. population density) their _mean_ needs to be
preserved.

## `gapminder` example

From the help of `gapminder` in package `gapminder` (by Jenny Bryan)
"by continent, which country experienced the sharpest 5-year drop
in life expectancy and what was the drop?"
```{r}
library(dplyr)
library(gapminder)
gapminder |>
   group_by(continent, country) |>
   select(country, year, continent, lifeExp) |>
   mutate(le_delta = lifeExp - lag(lifeExp)) |>
   summarize(worst_le_delta = min(le_delta, na.rm = TRUE)) |>
   filter(min_rank(worst_le_delta) < 2) |>
   arrange(worst_le_delta)
```
     
Now, can we show a map of yearly changes in life expectancy?

Step 1: match the table to country geometries:

```{r}
library(rnaturalearth)
library(sf)
ne = ne_countries(returnclass = "sf")
# Try to match country names:
m = match(gapminder$country, ne$admin)
(u = unique(gapminder$country[is.na(m)]))
```

So this is pretty ugly. We can solve it the hard way, manually
sorting out:

```{r}
n = as.character(ne$admin)
(conv = tribble(
		~old, ~new, 
		u[3], n[34],
		u[4], n[35],
		u[5], n[32],
        u[6], n[63],
		u[8], n[130],
		u[9], n[88],
		u[13], n[148],
		u[15], n[150],
		u[16], n[165],
		u[17], n[169],
		u[19], n[174]
))
library(stringr)
repl = conv$new
names(repl) = conv$old
gapminder$ncountry =
  str_replace_all(gapminder$country, repl)
m = match(gapminder$ncountry, ne$admin)
(un = unique(gapminder$country[is.na(m)]))
```
reducing the number of unmatched countries from 19 to 8.

Now we can do the big trick, and add a geometry column to `gapminder`:

```{r}
gapminder_sf = st_sf(gapminder, geometry = ne$geometry[m])
gapminder_sf$geometry
sum(is.na(m))
```

We see that the `NA` values in `m` have been filled with empty
geometries. Let's look at the geometries:
```{r}
st_geometry(gapminder_sf) |> plot()
```

Oops! We didn't see that from the table above! Also the question
didn't mention "of all countries with complete time series"...

The help from `gapminder` (can you find that?) says: "The
supplemental data frame ‘gapminder_unfiltered’ was not filtered
on ‘year’ or for complete data and has 3313 rows." Maybe we
should use that?

```{r}
gapminder_unfiltered$ncountry =
  str_replace_all(gapminder_unfiltered$country, repl)
m = match(gapminder_unfiltered$ncountry, ne$admin)
(un = unique(gapminder_unfiltered$country[is.na(m)]))
```
so there are now 25 rather than 9 we can't match to natural earth
polygons, and 

```{r}
gapminder_u_sf = st_sf(gapminder_unfiltered, geometry = ne$geometry[m])
gapminder_u_sf$geometry
sum(is.na(m))
st_geometry(gapminder_u_sf) |> plot()
```

looks much better. Which other countries are now included, but absent from `gapminder`?

```{r}
setdiff(unique(gapminder_unfiltered$ncountry), unique(gapminder$ncountry))
```

We have 45 countries _more_ than in `gapminder`, meaning that we
now have 45 - 25 + 9 = 29 countries _more_ matched with polygons.

We could now, for instance, create a map for the last measured year:
```{r}
gapminder_u_sf |> 
		filter(year == 2007) |> 
		select(lifeExp) |> 
		plot()
```

where South Sudan appears to be missing (it gained independence in 2011).

Assessing missing-ness from plotting is a good check to find big
ones (Russia), but not at all sufficient, since many countries are
small. How many countries go invisible on a plot like the one above?
Note that we plot country borders with thin lines (one pixel). 
Suppose that for a country to be visible with color
it must span at least, say 3 x 3 pixels.
We can obtain the device size in pixels by
```{r eval=FALSE}
dev.size("px")
```
and let's say it is 1000 by 1000. Then, the size of a single pixel
(when we do not have margins around the plot) at the equator is roughly
```{r}
library(units)
(pix_size = 2 * pi * set_units(s2::s2_earth_radius_meters(), m) / 1000)
```
and the number of the Natural Earth countries with an
area smaller than 9 equator-pixels is
```{r}
a = st_make_valid(ne) |> # the Natural Earth dataset is invalid on the sphere
	st_area() |> 
	na.omit()
sum(a < 9 * pix_size^2)
```

If we show the matrix of `lifeExp` by country and year,
```{r}
co = as.factor(gapminder_u_sf$ncountry)
minyear = min(gapminder_u_sf$year)
ye = gapminder_u_sf$year - minyear + 1
m = matrix(NA, length(levels(co)), length(unique(ye)))
m[cbind(co,ye)] = gapminder_u_sf$lifeExp
image(x = sort(unique(ye)) + minyear - 1, z = t(m))
```

where we see a clear pattern of (more) valid values for every
year ending in 2 or 7. We can show maps for each of those
years by:

```{r}
sel = gapminder_u_sf$year %% 5 == 2 # 2002, 2007 etc
ggplot() + geom_sf(data = gapminder_u_sf[sel,], aes(fill = lifeExp)) +
		facet_wrap(~year)
```

```{r}
ggplot() + geom_sf(data = gapminder_sf, aes(fill = lifeExp)) +
		facet_wrap(~year)
```


## raster data

Raster data data are used for various phenomena, but are in
particular suitable for phenomena that vary continuously over space,
such as elevation, temperature, air quality, and so on. Also imagery,
when georeferenced, is provided as raster data.

Raster data is usually regular, but may come in other
forms. Converting (projecting, reprojecting, unprojecting) may
cause curvilinear grids; regridding these is called _warping_,
may cause some data loss and is not invertible.

```{r out.width='70%',echo=FALSE}
knitr::include_graphics("https://keen-swartz-3146c4.netlify.app/sds_files/figure-html/rastertypes01-1.png")
```

Color imagery comes with three bands, satellite imagery, as shown
above, typically has more than three bands. 

## raster time series

Video is a time sequence of RGB images; satellite images also come as
repeated time series but at much lower temporal resolution. Weather
data ("nowcasts"), weather forecasts, climate predictions, land or
sea surface temperatures are all created, using models, to create
raster time series. Here is an example:

```{r}
w = system.file("nc/bcsd_obs_1999.nc", package = "stars") |>
    read_stars()
w
```

Where we see monthly data of precipitation (and temperature).
We also see that `x` and `y` do, but `time` does not have regular
intervals. From the data set alone, it is also not clear whether
monthly _total_ precipitation (and mean temperature) are given,
or values for the date shown (most likely: the former; this is
temporal _support_).

We can plot these data, and add some state boundary for reference by:

```{r}
nc = read_sf(system.file("gpkg/nc.gpkg", package="sf")) |>
	st_geometry()
nc.u = st_union(nc)
hook = function() plot(st_geometry(nc.u), add = TRUE, border = 'orange')
plot(w, hook = hook)
```

or by using `ggplot2`:

```{r}
ggplot() + geom_stars(data = w) + 
		geom_sf(data = st_cast(nc.u, "MULTILINESTRING"), col = 'orange') + 
		facet_wrap(~as.Date(time))
```

## image data cubes, image collections

Higher-dimensional data cubes are for instance obtained by remote
sensing imagery, where dimensions include `x`, `y` (raster), `band`
(color / wavelength) and `time`. Satellite imagery for larger areas
is often distributed as images (tiles) for different coordinate
reference systems, and with times of the overpass of satellite.
This means that _warping_ and aggregation is required to create a
**regular** data cube out of such image collections. A
recent blog entry on how to do this (reproducibly) is found
[here](https://r-spatial.org/r/2021/04/23/cloud-based-cubes.html).

Although much of this data is free-for-download, analysing this
data over a large area (even at low resolution) and/or over a longer
time span quickly makes the download option unfeasible.  Using cloud
platforms that already serve (fast) access to complete data archive
are the only alternative.

## Q&A

# Models

The goal of data analysis is often beyond descriptive statistics
and exploratory graphs, but may involve

* inference (estimation) of e.g. relationships between variables, or properties of the process that most likely generated the data 
* prediction of unobserved values (e.g. missing values in `gapminder`, spatial and/or temporal interpolation) from observations
* predicting properties that were not directly, or fully, observed (e.g. predicting land cover or land use from satellite images and ground truth observations)

Problems special to _spatial data_ often feed back to the sampling process and the goals:

1. Can analysis proceed design-based? Yes if some form of _random sampling_ was used to collect the data, inclusion probabilities are known, and statistics for the sampling units are required (and not e.g. spatial interpolation between samples)
2. If the answer is "no", analysis proceeds model-based, and if the data are spatially correlated, this correlation needs to be addressed in the analysis.

"Classical" modelling approaches (e.g. linear regression, but also most machine learning approaches) assume IID data: Independent, Identically Distributed. If analysis proceeds "model-based", data are not independent. If sampling is biased, observations are also not identically distributed.

## Point patterns

Wind turbine in Germany, e.g. from [here](https://opendata-esri-de.opendata.arcgis.com/datasets/esri-de-content::-onshore-windkraftanlagen-in-deutschland/explore?location=51.163621%2C10.453852%2C6.67) we can download the wind turbines as a GeoJSON file, which we will assume resides in directory `~/Downoads`

```{r,eval = file.exists("~/Downloads/_Onshore_Windkraftanlagen_in_Deutschland.geojson")}
wt_sf = read_sf("~/Downloads/_Onshore_Windkraftanlagen_in_Deutschland.geojson")
library(rnaturalearth)
library(tidyverse)
de = ne_countries(scale = 50, returnclass = "sf") |>
	filter(admin == "Germany") 
plot(st_geometry(de), border = 'red')
plot(st_geometry(wt_sf), add = TRUE, cex = .5)
```

If we create an interactive plot for this dataset with `mapview`, using
```{r}
library(mapview)
mapviewOptions(fgb = FALSE) # not needed locally
mapview(wt_sf)
```
we can zoom and click on points to see their attributes. We also see
that different color tones indicate overplotted, identical points;
there is no way of

We can create a `ppp` object from this by

```{r error=TRUE}
library(spatstat)
pp = st_geometry(wt_sf)
window = st_geometry(de) 
wt = as.ppp(c(window, pp))
```

Which won't work; we'll have to project these data first, e.g. to
UTM zone 32N. We also need to remove points with missing (empty)
geometries.
```{r}
crs = st_crs("EPSG:32632")
pp = st_transform(pp, crs)[!st_is_empty(pp)]
window = st_transform(window, crs)
wt = as.ppp(c(window, pp))
```

```{r}
plot(density(wt, bw = "SJ"))
plot(window, add = TRUE)
```

Of course, this density depends on the bandwidth, and we can create more
rough or smooth versions by setting it:

```{r}
plot(density(wt, sigma = 1e4))
plot(window, add = TRUE)
```

```{r}
plot(density(wt, sigma = 1e6))
plot(window, add = TRUE)
```

### Second order moments

The K function counts for a varying window size $r$ how many
additional points are found within a window of radius $r$ centered
at each point, and compares that to a completely spatially random
(CSR, Poisson) process.

```{r}
plot(Kest(wt))
```

One can compute an envelope around the CSR model (which is omitted here
because of excessive runtime)
```{r eval=FALSE}
plot(envelope(wt, Kest))
```

Another measure is the G-function, which estimates the nearest neighbour
distribution function from a point pattern.

```{r}
plot(Gest(wt))
```

We see that ther is a clear point mass at distance zero, indicating many
duplicates. We can remore duplicates using `unique()`, and see a strong
difference:

```{r}
plot(Gest(unique(wt)))
```

For this function, the envelope is quickly computed:
```{r}
plot(envelope(unique(wt), Gest))
```

and indicates we have a significant deviation from CSR (Poisson,
theoretical), with nearest neighbour distances much more abundant
then expected (clustering, attraction).


## Geostatistical data

Geostatistical analysis is often focused on the interpolation of
a spatially continuous variable (a _field_), from scattered point
observations. Consider the wind turbine data from the previous section,

```{r}
wt_sf$Power = as.numeric(gsub(",", ".", gsub("\\.", "", wt_sf$Gesamtleistung__MW_)))
plot(wt_sf["Power"], pch = 16, logz = TRUE, reset = FALSE)
plot(st_geometry(de), add = TRUE)
```

then we see scattered values, which we could _theoretically_
interpolate but the outcome would be meaningless: at unobserved
locations there are no wind turbines, and the power of a turbine
is not an indication of e.g.  wind potential (as they are all of
different size).

An example of a data set we could meaningfully interpolate is about
air quality, with annual mean NO2 concentrations, measured at rural
background stations. The data are from package `gstat`, but from a
not-yet-released version, so we read them directly from GitHub:

```{r}
url = "https://raw.githubusercontent.com/r-spatial/gstat/master/inst/external/no2.csv"
no2.sf = read.csv(url) |> 
    st_as_sf(coords = c("station_longitude_deg", "station_latitude_deg"),
			 crs = st_crs(4326))
plot(no2.sf["NO2"], pch = 16, reset = FALSE)
plot(st_geometry(de), add = TRUE)
```

```{r}
library(gstat)
v = variogram(NO2~1, no2.sf)
v.fit = fit.variogram(v, vgm(1, "Exp", 100, 1))
plot(v, v.fit, plot.numbers = TRUE, xlab = "distance (km)")
```

```{r}
library(stars)
grd = st_as_stars(st_bbox(de))
st_crs(grd) = st_crs(no2.sf)
k = krige(NO2~1, no2.sf, grd, v.fit)
plot(k["var1.pred"], breaks = "equal", reset = FALSE, axes = TRUE)
plot(st_geometry(de), add = TRUE, border = 'orange')
plot(st_geometry(no2.sf), col = 'green', add = TRUE, pch = 3)
```

Note that this procedure used ellipsoidal coordinates all the way,
and computed distances correctly nevertheless. In many cases, it
is more convenient (or appropriate) to carry out geostatistical
analyses using projected coordinates:

```{r}
no2.sf_utm = st_transform(no2.sf, st_crs("EPSG:32632"))
de_utm = st_transform(de, st_crs("EPSG:32632"))
v = variogram(NO2~1, no2.sf_utm)
v.fit = fit.variogram(v, vgm(1, "Exp", 10000, 1))
plot(v, v.fit, plot.numbers = TRUE, xlab = "distance (m)")
```

```{r}
grd_utm = st_as_stars(st_bbox(de_utm))
k = krige(NO2~1, no2.sf_utm, grd_utm, v.fit)
plot(k["var1.pred"], breaks = "equal", reset = FALSE, axes = TRUE)
plot(st_geometry(de_utm), add = TRUE, border = 'orange')
plot(st_geometry(no2.sf_utm), col = 'green', add = TRUE, pch = 3)
```

We can of course crop the map to the area of Germany

```{r}
plot(st_crop(k,de_utm)["var1.pred"], reset = FALSE)
plot(st_geometry(de_utm), add = TRUE, border = 'orange')
plot(st_geometry(no2.sf_utm), col = 'green', add = TRUE, pch = 3)
```

but in general field variables like air quality don't "stop" at
administrative boundaries.

## Lattice data

The following example was taken from Chapters 14-17 of [SDS](https://r-spatial.org/book).

The analysed variable is first round turnout proportion of registered voters in municipalities and Warsaw burroughs in the 2015 Polish presidential election.

```{r}
# install.packages("spDataLarge", repos = "https://nowosad.github.io/drat/", type = "source")
data(pol_pres15, package = "spDataLarge")
pol_pres15 |>
    subset(select = c(TERYT, name, types)) |>
    head()
library(tmap)
tm_shape(pol_pres15) + tm_fill("types")
```

We can compute Moran's I, a measure for spatial autocorrelation, when
we established neighbourhood relationships:

```{r}
library(spdep)
pol_pres15 |> poly2nb(queen = TRUE) -> nb_q
nb_q |> nb2listw(style = "B") -> lw_q_B
glance_htest <- function(ht) c(ht$estimate,
    "Std deviate" = unname(ht$statistic),
    "p.value" = unname(ht$p.value))
(pol_pres15 |>
        st_drop_geometry() |>
        subset(select = I_turnout, drop = TRUE) -> z) |>
    moran.test(listw = lw_q_B, randomisation = FALSE) |>
    glance_htest()
```

The book chapter (15) then further details this into local Moran's
I tests.

Spatial regression models are classical regression models extended
with effects that take care of spatial nature of the data, using
either

* a spatially correlated residual (e.g. characterised by a single
  correlation coefficient for first-order neighbours)
* a spatial autoregression model, where the (mean of) responses
  in neighbouring polygons is added as a regressor

Chapter 16 and 17 of [SDS](https://r-spatial.org/book) gives examples
of several of the usual approaches, for this.

With spatial time series, such as in `gapminder`, panel linear models
can be considered,

```{r}
library(plm)
plm(log(lifeExp) ~ log(gdpPercap), gapminder, index = c("country", "year")) |>
	summary()
```

but these entirely ignore spatial (neighbourhood) effects.
For _spatial_ panel linear models we follow [Milo & Piras,
2012](https://www.jstatsoft.org/article/view/v047i01); we must
first create a spatial weight matrix, which is row-standardised:

```{r}
gm = gapminder_sf |> filter(year == 2002) # or any other
wm = st_intersects(gm, gm) |> 
	as.matrix()
diag(wm) = 0 # remove self-intersections
dimnames(wm) = list(gm$country, gm$country)
# row-standardise:
wm_2 = apply(wm, 1, function(x) { s = sum(x); if (s == 0) 0*x else x/(sum(x)) }) |>
	t()
```

We need to remove countries with zero neighbours:

```{r}
wm_nn = which(apply(wm_2, 1, sum) == 0)
gmlistw = mat2listw(wm_2[-wm_nn, -wm_nn])
gapminder_nn = gapminder |> 
	filter(!country %in% names(wm_nn)) |>
	select(-continent) # so that column 1 and 2 are individual, time
```

and can then fit the most simple model by
```{r}
library(splm)
spml(log(lifeExp) ~ log(gdpPercap), gapminder_nn, listw = gmlistw) |>
	summary()
```

and a model that also has a spatial autoregressive component by
```{r}
spml(log(lifeExp) ~ log(gdpPercap), gapminder_nn, listw = gmlistw,
		model = "random", lag = TRUE) |>
	summary()
```

Note that it was not further examined whether this model makes
any sense, or the transformations; this is purely meant as an
illustration how to get `spml` to work.

## Q&A

```{r}
save.image()
sessionInfo()
```


